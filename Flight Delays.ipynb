{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1955c81e-c24a-44de-b1b9-2b121b571774",
   "metadata": {},
   "source": [
    "### Download dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8203a624-925b-4dca-9ec7-94098b5bd4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Must install kaggle first:\n",
    "#       Download .tar.gz file from Pypi\n",
    "#       Unzip compressed file\n",
    "#       Run setup using `python -m setup.py install`\n",
    "\n",
    "# Install additional packages\n",
    "!pip install opendatasets --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25ddd9-c0f0-4393-9988-d6a4328d09b1",
   "metadata": {},
   "source": [
    "### Downloading the data\n",
    "\n",
    "Download the data package from Kaggle to a temporary directory; the temp dir is created via `tempfile` and the dataset download is handled via `opendatasets`. Note Kaggle asks for a username and key -- type \"a\" into both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7446b7dd-629f-4b09-b381-27f2c02f239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import opendatasets as od\n",
    "\n",
    "# Create new temporary directory\n",
    "data_folder = tempfile.mkdtemp()\n",
    "\n",
    "# Download dataset into temporary directory\n",
    "# A more complete dataset can be found at 'https://www.kaggle.com/datasets/patrickzel/flight-delay-and-cancellation-dataset-2019-2023'\n",
    "dataset = 'https://www.kaggle.com/datasets/usdot/flight-delays'\n",
    "od.download(dataset, data_folder, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1841ee-b2f9-431e-907d-4584bc2019e8",
   "metadata": {},
   "source": [
    "### Data element insights\n",
    "\n",
    "The following categories are given in xx:yy - hour:minute format (e.g. 1536 means 3:36pm, 345 means 3:45am, 16 means 00:16am): \n",
    "- `SCHEDULED_DEPARTURE`\n",
    "- `DEPARTURE_TIME`\n",
    "- `SCHEDULED_ARRIVAL`\n",
    "- `ARRIVAL_TIME`\n",
    "- `WHEELS_OFF`\n",
    "- `WHEELS_ON`\n",
    "\n",
    "Other parameters are given in minutes (negative values indicate the actual time occurred before the scheduled time by the same magnitude):\n",
    "- `ARRIVAL_DELAY`\n",
    "- `DEPARTURE_DELAY`\n",
    "- `TAXI_IN`\n",
    "- `TAXI_OUT`\n",
    "- `SCHEDULED_TIME`\n",
    "- `ELAPSED_TIME`\n",
    "- `AIR_TIME`\n",
    "\n",
    "The `DISTANCE` parameter is given in miles.\n",
    "\n",
    "Specific definitions of some parameters are as follows:\n",
    "- `WHEELS_OFF`: The time point that the aircraft's wheels leave the ground.\n",
    "- `WHEELS_ON`: The time point that the aircraft's wheels touch on the ground.\n",
    "- `TAXI_OUT`: The time duration elapsed between departure from the origin airport gate and wheels off.\n",
    "- `TAXI_IN`: The time duration elapsed between wheels-on and gate arrival at the destination airport.\n",
    "- `AIR_TIME`: The time duration between wheels_off and wheels_on time.\n",
    "\n",
    "And, similarly, relationships between different data columns can be defined:\n",
    "- `ARRIVAL_TIME` = `WHEELS_ON` + `TAXI_IN`\n",
    "- `ARRIVAL_DELAY` = `ARRIVAL_TIME` - `SCHEDULED_ARRIVAL`\n",
    "- `DEPARTURE_TIME` = `WHEELS_OFF` - `TAXI_OUT`\n",
    "- `DEPARTURE_DELAY` = `DEPARTURE_TIME` - `SCHEDULED_DEPARTURE`\n",
    "- `ELAPSED_TIME` = `AIR_TIME` + `TAXI_IN` + `TAXI_OUT`\n",
    "- `AIR_TIME` = `WHEELS_ON` - `WHEELS_OFF`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642c053e-da80-4cbc-9bbf-0b308cb0bdfd",
   "metadata": {},
   "source": [
    "### Airlines dataset\n",
    "\n",
    "Let's look at the Airlines dataframe. In structure, it's pretty straight-forward, comprising two columns: the International Air Transport Association (IATA) code for each airline, and the name of the respective airline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75e38aa-dadb-4326-a74c-34a406f1dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read each CSV dataset\n",
    "airlines_path = os.path.join(data_folder,'flight-delays','airlines.csv')\n",
    "\n",
    "# Read each CSV into Pandas dataframe\n",
    "airlines = pd.read_csv(airlines_path,low_memory=False)\n",
    "\n",
    "print(airlines.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71acd8cd-8239-4ba0-b4f6-1691e7f8d8e7",
   "metadata": {},
   "source": [
    "### Airports dataset\n",
    "\n",
    "We need information on which airports the airlines operate from and where each airport is located. Each airport has an associated IATA code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fc11c3-dd47-41bf-bd75-e158e1d98d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read each CSV dataset\n",
    "airports_path = os.path.join(data_folder,'flight-delays','airports.csv')\n",
    "\n",
    "# Read each CSV into Pandas dataframe\n",
    "airports = pd.read_csv(airports_path,low_memory=False)\n",
    "\n",
    "print(airports.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111ead08-a863-4734-95d5-1a9f43d6423e",
   "metadata": {},
   "source": [
    "### Flights dataset\n",
    "\n",
    "The remaining file contains data for each flight described by the date of each flight, flight specifics (tail number, airline, and flight number), and the flight's origin and destination information. The data also include flight diversion, delay, and cancellation status. If the flights were delayed, the dataset provides the delay type and delay length. Note that January 1, 2015 (`MONTH`=1, `DAY`=1, `YEAR`=2015) was a Thursday; thus the first day of the week (`DAY_OF_WEEK`=1) corresponds to Monday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce808e51-ed0f-4832-8402-9ff1be85182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read each CSV dataset\n",
    "flights_path = os.path.join(data_folder,'flight-delays','flights.csv')\n",
    "\n",
    "# Read each CSV into Pandas dataframe\n",
    "flights = pd.read_csv(flights_path,low_memory=False)\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(flights.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4fa1e9-1651-4b15-a119-0842515e90e6",
   "metadata": {},
   "source": [
    "### Split data: Cancelled VS Non-cancelled (\"successful\")\n",
    "We'll start by splitting all recorded flights into cancelled flights and non-cancelled (or 'successful') flights, only focusing on the latter throughout the remainder of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39210b3c-b72b-4ad1-8a35-2ab5a45c1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into cancelled flights and uncancelled flights\n",
    "flown_flights = flights[flights['CANCELLED']==0]\n",
    "cancl_flights = flights[flights['CANCELLED']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaf4ad3-dda0-406c-bfef-e75bb608e638",
   "metadata": {},
   "source": [
    "### Cleaning / homogenizing flight data\n",
    "We'll then simply the resulting dataframe further:\n",
    "\n",
    "- Successful flights were not cancelled, so we'll remove the `CANCELLED` and `CANCELLATION_REASON` columns.\n",
    "- `TAIL_NUMBER` and `FLIGHT_NUMBER` will not impact the delay type, so we'll get rid of those.\n",
    "- `ARRIVAL_DELAY` accounts for `ARRIVAL_TIME` and `SCHEDULED_ARRIVAL`, so keep the former and remove the latter.\n",
    "- `ARRIVAL_TIME` accounts for `WHEELS_ON` and `TAXI_IN`, so keep the former and remove the latter.\n",
    "- `DEPARTURE_DELAY` accounts for `DEPARTURE_TIME` and `SCHEDULED_DEPARTURE`; keep the former, remove the latter.\n",
    "- `DEPARTURE_TIME` accounts for `WHEELS_OFF`, and `TAXI_OUT`; keep the former, remove the latter.\n",
    "- `ELAPSED_TIME` accounts for `AIR_TIME` and `DISTANCE` since longer distances require longer air time. Keep the former, remove the latter.\n",
    "- `ARRIVAL_DELAY` is likely the sum of the other delay types (`DEPARTURE`,`AIR_SYSTEM`,`SECURITY`,`LATE_AIRCRAFT`, and `WEATHER`) but we cannot be sure. Remove `ARRIVAL_DELAY` and define a new column to represent the delay sum.\n",
    "- Rearrange all columns to group `_DELAY` types .\n",
    "\n",
    "Coincidentally, dropping these eight columns also eliminates every column of a format other than minutes or miles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dbca86-09e3-48ff-ba6b-f6c70f413911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove cancellation-related columns\n",
    "clean_flown = flown_flights.drop(columns=['CANCELLED','CANCELLATION_REASON'])\n",
    "\n",
    "# Remove irrelevant flight information (tail, flight number)\n",
    "clean_flights = clean_flown.drop(columns=['TAIL_NUMBER','FLIGHT_NUMBER'])\n",
    "\n",
    "# Remove redundant arrival data\n",
    "clean_arrival = clean_flights.drop(columns=['ARRIVAL_TIME','SCHEDULED_ARRIVAL','WHEELS_ON','TAXI_IN'])\n",
    "\n",
    "# Remove redundant departure data\n",
    "clean_departure = clean_arrival.drop(columns=['DEPARTURE_TIME','SCHEDULED_DEPARTURE','WHEELS_OFF','TAXI_OUT'])\n",
    "\n",
    "# Remove redundant delay summation data\n",
    "clean_arrival = clean_departure.drop(columns=['ARRIVAL_DELAY','AIR_TIME','DISTANCE'])\n",
    "\n",
    "# Calculate the total delay for each flight\n",
    "clean_arrival['TOTAL_DELAY'] = clean_arrival['DEPARTURE_DELAY'] + clean_arrival['AIR_SYSTEM_DELAY'] + clean_arrival['SECURITY_DELAY'] + clean_arrival['AIRLINE_DELAY'] + clean_arrival['LATE_AIRCRAFT_DELAY'] + clean_arrival['WEATHER_DELAY']\n",
    "\n",
    "# Rearrange columns to group similar concepts\n",
    "clean_arrival = clean_arrival[['YEAR','MONTH','DAY','DAY_OF_WEEK','AIRLINE','ORIGIN_AIRPORT','DESTINATION_AIRPORT',\n",
    "                               'SCHEDULED_TIME','ELAPSED_TIME','DIVERTED', 'DEPARTURE_DELAY','AIR_SYSTEM_DELAY',\n",
    "                               'SECURITY_DELAY','AIRLINE_DELAY', 'LATE_AIRCRAFT_DELAY','WEATHER_DELAY','TOTAL_DELAY']]\n",
    "\n",
    "# Show current dataframe\n",
    "clean_arrival.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d67c0de-90f2-463e-90c6-2849f6b99793",
   "metadata": {},
   "source": [
    "### Handling date/time\n",
    "\n",
    "Parameters like \"YEAR\", \"MONTH\", and \"DAY\" can be combined into \"Day_of_Year\", which has the added benefit of tracking seasonality and possibly interesting metrics later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dbcb9b-d3ac-45b7-9a96-00654d588dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Combine YEAR, MONTH, and DAY into a single column\n",
    "clean_arrival['DAY_OF_YEAR'] = [datetime(y,m,d).timetuple().tm_yday for y,m,d in zip(clean_arrival['YEAR'],\n",
    "                                                                                     clean_arrival['MONTH'],\n",
    "                                                                                     clean_arrival['DAY'])]\n",
    "# Consolidate calendar information\n",
    "clean_doy = clean_arrival.drop(columns=['YEAR','MONTH','DAY'])\n",
    "\n",
    "# Print output metrics\n",
    "print('\\n# rows:', clean_doy.shape[0], '\\n# categories:',clean_doy.shape[1])\n",
    "print('\\n',clean_doy.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9853c780-6e9c-4f77-b105-886cc4b946bd",
   "metadata": {},
   "source": [
    "### Airline consistency\n",
    "\n",
    "We still need to handle airline specification. Currently, airlines are specified by uninterpretable 2-letter acronyms (ex. \"NK\"). We'll bring in other datasets to convert the acronyms to human-readable labels (ex. 'Spirit Airlines')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb925e-2ac9-4bcd-8ca9-c9a2e69cbf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dictionaries from dataframes\n",
    "airline_dict = dict(zip(airlines.IATA_CODE, airlines.AIRLINE))\n",
    "\n",
    "# Update airline name\n",
    "airline_name = []\n",
    "[airline_name.append(airline_dict[u]) for u in clean_doy['AIRLINE']]\n",
    "clean_doy['AIRLINE'] = airline_name\n",
    "\n",
    "print(clean_doy.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560d9bc2-5f69-4f86-a14c-5809907f3042",
   "metadata": {},
   "source": [
    "### Origin / Destination inconsistencies\n",
    "\n",
    "Airport identifiers are also problematic. For example, `ORIGIN_AIRPORT` and `DESTINATION_AIRPORT` both contain 3-letter IATA airport codes (ex. \"LAX\") and 5-digit FAA numerical codes (ex. 12789). Both formats are valid, but their formats are inconsistent.\n",
    "\n",
    "The numerical FAA airport identifiers are not included in the original Kaggle dataset, but they can be connected using two additional datafiles provided by the Bureau of Transportation Statistics (`L_AIRPORT_ID.csv` and `L_AIRPORT.csv`). The first links the numerical ID value to an aiport name, and the second links the airport name to its alphabetic airport identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e33179-f913-4276-957e-a5aa3f489c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for files connecting airport IDs and IATA codes\n",
    "id_path = os.path.join(os.getcwd(),'flight-delays','L_AIRPORT_ID.csv')\n",
    "code_path = os.path.join(os.getcwd(),'flight-delays','L_AIRPORT.csv')\n",
    "\n",
    "# Read CSV files as dataframes\n",
    "airport_id = pd.read_csv(id_path, low_memory=False)\n",
    "airport_code = pd.read_csv(code_path, low_memory=False)\n",
    "\n",
    "# Merge dataframes on Airport name\n",
    "merged_codes = pd.merge(airport_id, airport_code, on=\"Description\")\n",
    "\n",
    "# Rename dataframe columns\n",
    "merged_codes = merged_codes.rename(columns={\"Code_x\": \"FAA_ID\", \"Code_y\": \"IATA\"})\n",
    "\n",
    "# Show new results\n",
    "merged_codes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86852358-4e1f-49cc-b3c2-d5415d85e271",
   "metadata": {},
   "source": [
    "With this \"Rosetta stone\" in hand, we correct the `flights` dataset to contain only IATA codes in both columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0678dd63-0dd3-49ca-9777-3224a2c942fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nOld airport origin list:\\n', clean_doy.ORIGIN_AIRPORT.unique())\n",
    "\n",
    "# Create dictionary linking airport IDs with IATA codes\n",
    "orig_airport_dict = dict(zip(merged_codes.FAA_ID, merged_codes.IATA))\n",
    "\n",
    "# Match airport ID numbers with IATA codes\n",
    "orig_airport = []\n",
    "for a in clean_doy.ORIGIN_AIRPORT:\n",
    "    if str(a).isdecimal():\n",
    "        orig_airport.append(orig_airport_dict[int(a)])\n",
    "    else:\n",
    "        orig_airport.append(a)\n",
    "\n",
    "# Replace dataframe column\n",
    "flown_origin = clean_doy.copy()\n",
    "flown_origin.ORIGIN_AIRPORT = orig_airport\n",
    "\n",
    "# Test if all elements in ORIGIN_AIRPORT are alphabetic identifiers\n",
    "print('\\nNew airport origin list:\\n',flown_origin.ORIGIN_AIRPORT.unique())\n",
    "print(\"\\nAll elements only contain letters:\",all([x.isalpha() for x in flown_origin.ORIGIN_AIRPORT]),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e1995-9435-4e66-bf6b-272fab3a7dfc",
   "metadata": {},
   "source": [
    "Repeat the same steps to convert `DESTINATION_AIRPORT` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73c061-2f99-4832-97e4-fd32e55ec668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match airport ID numbers with IATA codes\n",
    "dest_airport = []\n",
    "for d in flown_origin.DESTINATION_AIRPORT:\n",
    "    if str(d).isdecimal():\n",
    "        dest_airport.append(orig_airport_dict[int(d)])\n",
    "    else:\n",
    "        dest_airport.append(d)\n",
    "\n",
    "# Replace dataframe column\n",
    "flown_dest = flown_origin.copy()\n",
    "flown_dest.DESTINATION_AIRPORT = dest_airport\n",
    "\n",
    "# Test if all elements in DESTINATION_AIRPORT are alphabetic identifiers\n",
    "print(\"\\nAll elements only contain letters:\",all([x.isalpha() for x in flown_dest.DESTINATION_AIRPORT]),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d09b977-7b0c-43eb-9512-9eb765d9539d",
   "metadata": {},
   "source": [
    "Manually remove outliers in `DESTINATION_AIRPORT`. Specifically, the airport \"BSM\" was recently updated to \"AUS\", and this switch does not exist in our current lookup tables so we make the change manually. Then we check there are no more list differences between our known airports and those listed in our agnostically processed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d494f-9173-4dcd-8302-f111030efb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show difference between `flights` IATA codes and `airports` IATA codes\n",
    "print('\\nAirport list differences:')\n",
    "print('   Before substitution:', len(list(set(flown_dest.ORIGIN_AIRPORT) - set(airports.IATA_CODE))))\n",
    "\n",
    "# Handle each IATA code change\n",
    "flown_dest.ORIGIN_AIRPORT = flown_dest.ORIGIN_AIRPORT.replace('BSM','AUS')\n",
    "flown_dest.DESTINATION_AIRPORT = flown_dest.DESTINATION_AIRPORT.replace('BSM','AUS')\n",
    "\n",
    "# Show output\n",
    "print('    After substitution:', len(list(set(flown_dest.ORIGIN_AIRPORT) - set(airports.IATA_CODE))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b502946-1bde-48ad-a517-94698e13cf96",
   "metadata": {},
   "source": [
    "### Add information to dataframes based on IATA codes\n",
    "\n",
    "Based on our knowledge of each airport's location, we can define latitude and longitude for each origin and destination airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b9da7f-f831-474a-82d5-eb930b5d333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Copy the existing dataframe before adding information\n",
    "flown_lla = flown_dest.copy()\n",
    "\n",
    "# Create dictionaries comprising origin airport, latitude, and longitude\n",
    "lats_dict = dict(zip(airports.IATA_CODE, airports.LATITUDE))\n",
    "lons_dict = dict(zip(airports.IATA_CODE, airports.LONGITUDE))\n",
    "\n",
    "# ---------- Add Latitude and Longitude for Origin ----------\n",
    "# Append matching lat,lon for each Origin_Airport\n",
    "olats,olons = [],[]\n",
    "[olats.append(lats_dict[u]) for u in flown_lla['ORIGIN_AIRPORT']]\n",
    "[olons.append(lons_dict[u]) for u in flown_lla['ORIGIN_AIRPORT']]\n",
    "\n",
    "# Add origin location to dataframe\n",
    "flown_lla['ORIGIN_LAT'] = olats\n",
    "flown_lla['ORIGIN_LON'] = olons\n",
    "\n",
    "# ---------- Add Latitude and Longitude for Destination ----------\n",
    "# Append matching lat,lon for each Destination_Airport\n",
    "dlats,dlons = [],[]\n",
    "[dlats.append(lats_dict[u]) for u in flown_lla['DESTINATION_AIRPORT']]\n",
    "[dlons.append(lons_dict[u]) for u in flown_lla['DESTINATION_AIRPORT']]\n",
    "\n",
    "# Add origin location to dataframe\n",
    "flown_lla['DESTINATION_LAT'] = dlats\n",
    "flown_lla['DESTINATION_LON'] = dlons\n",
    "\n",
    "# Show all columns when printing dataframes\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Since the LLA values make the airport names redundant, ORIGIN_AIRPORT and DESTINATION_AIRPORT can be removed\n",
    "flown_lla = flown_lla.drop(columns=['ORIGIN_AIRPORT','DESTINATION_AIRPORT'])\n",
    "print(flown_lla.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d210be-de73-486d-9748-1cbfe2a899f2",
   "metadata": {},
   "source": [
    "The last troublesome issue is the `NaN` values in our `_DELAY` columns; since our ML model will be used to predict the `_DELAY` type, this is a big deal. To avoid any incorrect assumptions, we'll remove all rows where any type of `_DELAY` value is `NaN` (Not a Number). Based on the remaining `_DELAY` values, we iterate through each flight and figure out which delay is the leading cause of the flight's delay, but we'll doing so knowing that when no delays are recorded, the `MAX_DELAY` values are set accordingly.\n",
    "\n",
    "#### A discussion on removing NaN delay values\n",
    "This topic turns into its own discussion. While further investigation reveals that if all NaN values are removed from each `_DELAY` column, there are no more flights in the remaining dataset that experience a culmination of 0.0 errors for each delay type -- that is, there are no \"perfect\" on-time flights left. It is tempting to assume that flights containing `NaN` values for each `_DELAY` indicates that there were no delays, and these values can be set to 0. But this cannot be the case, since the minimum and maximum values for `DEPARTURE_DELAY` are -82.0 and 1988.0, respectively within the existing dataframe, and the `DEPARTURE_DELAY`s had to be due to some reason (ex. weather, airlines, late aircraft, etc.), and there is no corresponding match. Additionally, when no delay occurs in any of the `_DELAY` columns, the values are often listed as `0.0` instead of `NaN`, so it is unsafe to assume that `NaN` values indicate `0.0` for all `_DELAY` columns.\n",
    "\n",
    "After these changes, we are left with 1,063,439 flights characterized by 19 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb6f1b-5f22-4336-9ef4-a33e4081280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Drop rows with NaN values in _DELAY columns\n",
    "flown_noNA = flown_lla.dropna(subset = ['AIR_SYSTEM_DELAY','SECURITY_DELAY','AIRLINE_DELAY','LATE_AIRCRAFT_DELAY','WEATHER_DELAY'])\n",
    "\n",
    "# Find max delay type in each row\n",
    "sub_df = flown_noNA[['AIR_SYSTEM_DELAY','SECURITY_DELAY','AIRLINE_DELAY','LATE_AIRCRAFT_DELAY','WEATHER_DELAY']]\n",
    "max_delay = sub_df.idxmax(axis=1)\n",
    "flown_noNA.loc[:,'MAX_DELAY'] = max_delay\n",
    "\n",
    "# Ensure no delay values are accurately recorded\n",
    "num_noDelay = len(flown_noNA[\n",
    "                          (flown_noNA['AIR_SYSTEM_DELAY'] == 0.0) &\n",
    "                          (flown_noNA['SECURITY_DELAY'] == 0.0) &\n",
    "                          (flown_noNA['AIRLINE_DELAY'] == 0.0) &\n",
    "                          (flown_noNA['LATE_AIRCRAFT_DELAY'] == 0.0) &\n",
    "                          (flown_noNA['WEATHER_DELAY'] == 0.0)\n",
    "                          ])\n",
    "print('Number of flights with 0 delays: ',num_noDelay)\n",
    "\n",
    "# Show sum for each max_delay column value\n",
    "print('\\nNumber of delay counts for each type:\\n',flown_noNA['MAX_DELAY'].value_counts())\n",
    "\n",
    "# Show head of remaining dataframe\n",
    "print('\\nRemaining data shape: ',flown_noNA.shape)\n",
    "print('\\n',flown_noNA.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38bbbf3-824f-47b8-9156-0ed60b24ca6f",
   "metadata": {},
   "source": [
    "# Initial data exploration\n",
    "\n",
    "   Let's begin by casting a very wide analysis net by plotting histograms of all features within the dataset to get an idea of their distributions. Note that some parameters (`INDEX`, `FLIGHT_NUMBER`, `DIVERTED`) do not tell us much, while other plots tell us a great deal (ex. `DAY_OF_WEEK`, `SCHEDULED_TIME`, `TOTAL_DELAY`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673ccf6b-9947-4b0a-aec5-423efcc396ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "flown_noNA.hist(bins=100, figsize=(20,15))\n",
    "print(flown_noNA.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e26089c-4921-45a1-9758-460b9f0a07ec",
   "metadata": {},
   "source": [
    "## Delay time VS. Airlines\n",
    "\n",
    "   Let's take this one step further and dig into the `TOTAL_DELAY` as a function of airlines. We'll first begin by looking at the total delay time for each airline. To do so, we will plot the total delay time for each airline on the same plot using the Seaborn package. By sorting the values in descending order, airlines with the highest delays will listed first, and airlines with the shortest delays will be listed last. To handle extreme cases (e.g. outliers), we will plot the logarithm of the total delay time, and sort on the log(Delay Time) values.\n",
    "\n",
    "   The Seaborn plot below demonstrates in 2015, American Airlines flights experienced the highest delay time of the 14 airlines included in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569e935e-33cd-43d9-98ea-37d0430285a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flown_noNA.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48171c8b-5869-413e-9e92-79a93e3010d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"dark\", rc={'figure.figsize':(11,5)})\n",
    "\n",
    "# Pre-emptively sort dataframe\n",
    "sorted_delay = flown_noNA.sort_values(by='TOTAL_DELAY',ascending=False).reset_index()\n",
    "descending_delays = sorted_delay['AIRLINE'].unique()\n",
    "\n",
    "# Generate plot of delays sorted by decreasing delay\n",
    "log_delay = np.log(sorted_delay['TOTAL_DELAY'])\n",
    "delay_plot = sns.violinplot(sorted_delay, x=\"AIRLINE\", y=log_delay, order=descending_delays)\n",
    "delay_plot.set_xticklabels(delay_plot.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "plt.xlabel(\"Airline\")\n",
    "plt.ylabel(\"log(Total delay)\")\n",
    "\n",
    "# Show head of dataframe\n",
    "print(sorted_delay.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cf110c-18a5-4874-8dc5-ec59d0c34d24",
   "metadata": {},
   "source": [
    "## Delay time VS Day of the Week\n",
    "\n",
    "   In a similar manner, Seaborn can be used to plot the log(Total Delay) values for each day of the week; keep in mind that in this dataset, `DAY_OF_WEEK`=1 corresponds to a Monday.\n",
    "\n",
    "   The plot below demonstrates that traveling on the fifth day of the week -- Friday -- is associated with the longest flight delays overall. Following Friday, Saturday and Sunday are associated with the next longest flight delays, with weekdays bringing up the lowest delays. Since Friday and the weekends are generally associated with higher volume traffic in aiports, we can reasonably associate the increase in the number of flights with the increase in delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52472430-a641-420c-9bbb-1a1f0b0fc27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-emptively sort dataframe\n",
    "descending_DayOfWeek = sorted_delay['DAY_OF_WEEK'].unique()\n",
    "\n",
    "# Generate plot of delays sorted by decreasing delay\n",
    "delay_plot = sns.violinplot(sorted_delay, x=\"DAY_OF_WEEK\", y=log_delay, order=descending_DayOfWeek)\n",
    "delay_plot.set_xticklabels(delay_plot.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "plt.xlabel(\"Day of Week\")\n",
    "plt.ylabel(\"log(Total delay)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582c628-6f2e-4325-bae4-f3fd8f2a2fa3",
   "metadata": {},
   "source": [
    "   But is this really the case? To make an association between the number of flights and the total length of flight delays, we'd need to examine the number of flights recorded on each day of the week. The number of flights can be plotted against the day of the week using the dataframe's `sum` function.\n",
    "\n",
    "   Interestingly, the largest number of flights within the dataset occurred on Thursday (DoW=4), then Monday (DoW=1), *then* Friday (DoW=5). Based on this comparison, we can say that the length of the flight delays is not directly associated with the *number* of flights on a given day, but rather some other factor. Of course, we must keep in mind that these conclusions are reached on a dataset of decreased volume based on the cleaning we performed at the beginning of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db5a6d-1287-4559-a249-f644a02acf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_flights = [(sorted_delay['DAY_OF_WEEK']==x).sum() for x in np.arange(1,8)]\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.bar(np.arange(1,8), num_flights)\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Number of flights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d400bb9-39eb-477b-a3cc-349d0178ea89",
   "metadata": {},
   "source": [
    "## Delay cause\n",
    "\n",
    "   Alternatively, we can query the database directly to find the root cause of the delays. We can use the cleaned dataset to plot the delay type according to the database for the leading offender in flight delays.\n",
    "\n",
    "   The results illustrate that \"Airline\" is the leading cause, followed by late aircraft then weather. While not terribly helpful, the plot does show that late flights and weather are amongth the top three causes of flight delays. Nonetheless, questions remain: What is constituted as an \"Airline delay\"? What causes \"late aircraft\"? How are weather and late aircraft associated? These are questions that remain outside the scope of this course given our interest in developing an AI/ML approach to data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d76a617-e95a-46e4-81e2-a5c895441f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plot of delay type for most delayed airline\n",
    "largest_delay_airline = sorted_delay['AIRLINE'][0]\n",
    "largest_delay_by_airline = sorted_delay[sorted_delay['AIRLINE']==largest_delay_airline]\n",
    "\n",
    "# Retain only flight delay data\n",
    "trimmed_df = largest_delay_by_airline[['AIR_SYSTEM_DELAY','SECURITY_DELAY','AIRLINE_DELAY','LATE_AIRCRAFT_DELAY','WEATHER_DELAY','TOTAL_DELAY']]\n",
    "delay_melt = trimmed_df.melt('TOTAL_DELAY', var_name='TYPE', value_name='LENGTH')\n",
    "\n",
    "# Sort data by delay length\n",
    "sorted_by_delay_len = delay_melt.sort_values(by='LENGTH',ascending=False).reset_index()\n",
    "sorted_by_delay_len['TYPE'] = [x.replace('_DELAY','').replace('_',' ').capitalize() for x in sorted_by_delay_len['TYPE']]\n",
    "descending_Type = sorted_by_delay_len['TYPE'].unique()\n",
    "sorted_by_delay_len['LENGTH'] = np.log(sorted_by_delay_len['LENGTH'])\n",
    "\n",
    "# Display violin plot of delay type sorted by decreasing delay length\n",
    "delay_plot = sns.violinplot(sorted_by_delay_len, x=\"TYPE\", y=\"LENGTH\",order=descending_Type)\n",
    "plt.xlabel(\"Delay type\")\n",
    "plt.ylabel(\"log(Delay length)\")\n",
    "plt.title(largest_delay_airline + \" delay causes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828db926-9a51-4d3b-9193-bf1aaa5b12db",
   "metadata": {},
   "source": [
    "Other detail-oriented analyses could also be done:\n",
    "- Delay causes across each airline\n",
    "- Delay length different geographic regions\n",
    "- Delay cause across different geographic regions\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f082c-f853-47b1-ace9-0bcdf4e479b6",
   "metadata": {},
   "source": [
    "# Imputation\n",
    "\n",
    "One way to resolve the missing data issue might be to use SciKit Learn's `impute` function to predict `NaN` datapoints based on a K-Nearest Neighbor algorithm. Although this method is shown below, it takes a very long time to execute given the size of our dataframe. Therefore, we'll eliminate rows with NaN values to see what size of dataframe we end up with. Nonetheless, code to impute the missing values is retained here for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446ef805-fd0e-4674-835c-0beb2f29b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# create a temporary copy of the dataset\n",
    "flights_temp = flown_no_sum.copy()\n",
    "\n",
    "# We'll have trouble with string values; remove for now\n",
    "flights_temp = flights_temp.drop(columns=['AIRLINE','ORIGIN_AIRPORT','DESTINATION_AIRPORT'])\n",
    "\n",
    "# retrieve columns with numerical data; will exclude the ocean_proximity column since the datatype is object; other columns are float64\n",
    "columns_list = [col for col in flights_temp.columns if flights_temp[col].dtype != 'str']\n",
    "\n",
    "# extract columns that contain at least one missing value\n",
    "new_column_list = [col for col in flights_temp.loc[:, flights_temp.isnull().any()]]\n",
    "\n",
    "# update temp dataframe with numeric columns that have empty values\n",
    "flights_temp = flights_temp[new_column_list]\n",
    "\n",
    "# initialize KNNImputer to impute missing data using machine learning\n",
    "knn = KNNImputer(n_neighbors = 3)\n",
    "\n",
    "# fit function trains the model \n",
    "knn.fit(flights_temp)\n",
    "\n",
    "# Transform the data using the model; applies the transformation model (ie knn) to data\n",
    "array_Values = knn.transform(flights_temp)\n",
    "\n",
    "# convert the array values to a dataframe with the appropriate column names\n",
    "flights_temp = pd.DataFrame(array_Values, columns = new_column_list)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1a95d4-612d-4ade-9502-d1ec3de6f805",
   "metadata": {},
   "source": [
    "### Extract largest delay type\n",
    "\n",
    "Ultimately, our Machine Learning model will attempt to predict the largest delay type for a given flight provided all other flight parameters. Therefore, we need to drop `NaN` values within the _DELAY columns, and isolate the largest delay type for each flight. This will allow us to count the frequency of each delay type to ensure the ML model can utilize evenly distributed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f047a8-7ef3-44c2-a08d-b7459b3842d4",
   "metadata": {},
   "source": [
    "## Cancelled flight analysis\n",
    "We'll now start cleaning and analyzing the cancelled flights. \n",
    "\n",
    "We can begin cleaning the dataframe by saving useful data columns by replacing confusing data with informal data. For instance, `CANCELLATION_REASONS` A, B, C, and D stand for \"Airline\", \"Weather\", \"National Air System\", and \"Security\", respectively. In this context, `NaN` likely indicates the flight was not cancelled, so these values can be replaced by \"None\" (as a string). We do this knowing we can use ML methods later on to categorize flights based on the cancellation reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27dda09-879d-452c-8260-b353710f070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove _DELAY columns from cancelled flights, and make cancellation reasons readable\n",
    "#clean_cancl = cancl_flights.drop(columns=['ARRIVAL_DELAY','DIVERTED','AIR_SYSTEM_DELAY','SECURITY_DELAY','AIRLINE_DELAY','LATE_AIRCRAFT_DELAY','WEATHER_DELAY'])\n",
    "#clean_cancl = clean_cancl.drop(columns=['DEPARTURE_TIME','DEPARTURE_DELAY','TAXI_OUT','WHEELS_OFF','ELAPSED_TIME','AIR_TIME','WHEELS_ON','TAXI_IN'])\n",
    "#clean_cancl = clean_cancl.drop(columns=['ARRIVAL_TIME'])\n",
    "\n",
    "## Make cancellation reason intelligible\n",
    "#clean_cancl.loc[:,'CANCELLATION_REASON'] = clean_cancl['CANCELLATION_REASON'].map({'A': 'Airline', 'B': 'Weather','C':'National Air System', 'D':'Security'}) \n",
    "#clean_cancl['CANCELLATION_REASON'].unique()\n",
    "\n",
    "#cancl_nulls = clean_cancl.isnull()\n",
    "#cancl_nulls_sum = cancl_nulls.sum().to_dict()\n",
    "#print('Size of cancelled flight dataframe: ',clean_cancl.shape)\n",
    "#print('\\nNaN values in each dataset:\\n',clean_cancl.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc10567-1437-4353-bae7-8f5d634cd28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show total number of data points\n",
    "#print('\\nTotal number of data points:',clean_cancl.shape[0])\n",
    "#\n",
    "## Query for null parameters in dataset\n",
    "#print('\\nNaN values in each dataset:\\n',clean_cancl.isnull().sum())\n",
    "#nulls_df = clean_cancl.isnull()\n",
    "#null_sum = nulls_df.sum().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b21469-c039-4109-bb7d-86ab7666d9ca",
   "metadata": {},
   "source": [
    "The column containing the most NaN values appears to be \"CANCELLATION REASON\". We'll identify this column programmatically and remove it from the dataframe. Then, we'll remove all other rows that contain NaN values. This will allow us to work with a dataframe void of NaN values with just over one million datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd3c78a-723e-40ba-a9d5-b1b977c0c1d3",
   "metadata": {},
   "source": [
    "Save information for Cancellation Reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19fc54d-3cc9-4a81-8f3a-4baf52cb9533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_flights['CANCELLATION_REASON'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6a1dfe-f47a-4220-ba15-cc6f4e1f80e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find max null set, remove column\n",
    "#max_null_count = max(nulls_df.sum())\n",
    "#res = [key for key in null_sum if null_sum[key] == max_null_count]\n",
    "#ncr_df = flights.drop(columns=res)\n",
    "#print(\"\\nLargest set with NaN:\",res[0])\n",
    "\n",
    "## Drop all other rows containing NaN\n",
    "#clean_flights = ncr_df.dropna().reset_index()\n",
    "#print(\"\\nTotal remaining NaN values:\", sum(clean_flights.isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac70717-6e3b-4d76-a69d-770076af3561",
   "metadata": {},
   "source": [
    "# AI/ML Analysis\n",
    "\n",
    "With some rudimentary analysis in the background, we'll now begin our dataset analysis using AI/ML. Specifically, we'll try to develop a model to categorize the *type* of delay experienced by a flight given the timing of the flight (day of week, year of day), airline, and other factors.\n",
    "\n",
    "### Parameter relationships\n",
    "Plotting the correlation of each numerical value (`numeric_only=True`) in the dataframe shows us how strongly each variable is related to other variables within the same dataframe. We immediately notice the `DIVERTED` dataset has null correlation; further investigation shows this is because there is one unique value in `DIVERTED` (0). We'll remove it for now.\n",
    "\n",
    "### Create categorization\n",
    "To do this, we'll add one more column combining all five `_DELAY` columns into one column that describes the maximum delay experienced for any given flight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66665230-5466-47ae-92a7-45a5fdac0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a graphical correlation matrix for each pair of columns in the dataframe\n",
    "corr1 = sorted_delay.corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "sns.heatmap(corr1, annot=False)\n",
    "sns.set(font_scale=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Remove `DIVERTED`\n",
    "after_clean = sorted_delay.drop(columns=['index','DIVERTED'])\n",
    "\n",
    "# Plot a graphical correlation matrix for each pair of columns in the dataframe\n",
    "corr2 = after_clean.corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "sns.heatmap(corr2, annot=False)\n",
    "sns.set(font_scale=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1cb22a-42fe-4898-b8c9-19141d25f40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlation between numerical values and airline names\n",
    "airlines = after_clean['AIRLINE'].unique()\n",
    "indices = np.arange(len(airlines))\n",
    "airline_key = zip(airlines,indices)\n",
    "\n",
    "# Show corresponding values\n",
    "print(list(airline_key))\n",
    "\n",
    "# Show result beforehand\n",
    "print('\\nUnique airlines before substitution:\\n',after_clean['AIRLINE'].unique())\n",
    "\n",
    "for k in np.arange(len(airlines)):\n",
    "    after_clean.loc[after_clean['AIRLINE'] == airlines[k], 'AIRLINE'] = k\n",
    "\n",
    "# Show result afterward\n",
    "print('\\nUnique airlines after substitution:\\n',after_clean['AIRLINE'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ebb3e-75f9-41df-9f60-30d53eb6634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(after_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5421e14-8de7-458c-ac61-0ca2b848fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlation between numerical values and airline names\n",
    "delays = after_clean['MAX_DELAY'].unique()\n",
    "indices = np.arange(len(delays))\n",
    "delay_key = zip(delays,indices)\n",
    "\n",
    "# Create unique labels for ML characterization\n",
    "uniqlbl = [x.split('_')[0] for x in after_clean['MAX_DELAY'].unique()]\n",
    "\n",
    "for k in np.arange(len(delays)):\n",
    "    after_clean.loc[after_clean['MAX_DELAY'] == delays[k], 'MAX_DELAY'] = k\n",
    "\n",
    "# Remove any final NaN values from dataframe\n",
    "clean_noNA = after_clean.dropna()\n",
    "\n",
    "print(\"\\nFinal dataset size:\\n\",clean_noNA.shape)\n",
    "print('\\nNaN values in final array:\\n',clean_noNA.isnull().values.any(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c51b902-517a-4f27-a330-24dc2d606b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqlbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26004d-15f9-49b2-b39b-75faeee61a8f",
   "metadata": {},
   "source": [
    "### Split the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59247af-2f54-4f5a-8431-ff9125e03184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "# Splitting the data into independent and dependent variables\n",
    "X = clean_noNA.drop(columns=['MAX_DELAY']).values\n",
    "y = clean_noNA['MAX_DELAY'].values\n",
    "feature_list = list(clean_noNA.drop(columns=['MAX_DELAY']).columns)\n",
    "\n",
    "# Creating the Training and Test set from data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "lab = preprocessing.LabelEncoder()\n",
    "y_train = lab.fit_transform(y_train)\n",
    "y_test = lab.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b105af26-3dc0-4845-a69f-33ca56dcf439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, log_loss, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# --------- PLOT TOP ROW OF CLASSIFIERS ---------\n",
    "classifiers = {\n",
    "    \"Dec. Tree\": DecisionTreeClassifier(),\n",
    "    \"Rnd Forest\": RandomForestClassifier(n_estimators = 25, criterion = 'gini'),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Log. Reg.\": LogisticRegression(),\n",
    "}\n",
    "\n",
    "f, axes = plt.subplots(1, 4, figsize=(12,6), sharey='row')\n",
    "\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "for i, (key, classifier) in enumerate(classifiers.items()):\n",
    "    y_pred = classifier.fit(X_train, y_train).predict(X_test)\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(cf_matrix,\n",
    "                                  display_labels=uniqlbl,\n",
    "                                 )\n",
    "    disp.plot(ax=axes[i], xticks_rotation=45)\n",
    "    disp.ax_.set_title(key)\n",
    "    disp.im_.colorbar.remove()\n",
    "    disp.ax_.set_xlabel('')\n",
    "    if i!=0:\n",
    "        disp.ax_.set_ylabel('')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.40, hspace=0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d1162a-ee29-4f8e-b609-9d23a4828e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "classifiers = {\n",
    "    \"Dec. Tree\": DecisionTreeClassifier(),\n",
    "    \"Rnd Forest\": RandomForestClassifier(n_estimators = 25, criterion = 'gini'),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Log. Reg.\": LogisticRegression(),\n",
    "}\n",
    "\n",
    "# Logging for Visual Comparison\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "for i, (key, classifier) in enumerate(classifiers.items()):\n",
    "    y_pred = classifier.fit(X_train, y_train).predict(X_test)\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    #name = classifier.__class__.__name__    \n",
    "    name = key\n",
    "    train_predictions = classifier.predict(X_test)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    train_predictions = classifier.predict_proba(X_test)\n",
    "    ll = log_loss(y_test, train_predictions)\n",
    "    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n",
    "    log = pd.concat([log, log_entry], ignore_index=True)\n",
    "\n",
    "\n",
    "# Image bar charts for Accuracy and Log Loss\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(15,2))\n",
    "plt.subplot(1,2,1)\n",
    "plt.style.use('dark_background')\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")\n",
    "plt.xlabel('Accuracy %')\n",
    "plt.title('Classifier Accuracy')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Log Loss', y='Classifier', data=log, color=\"g\")\n",
    "plt.xlabel('Log Loss')\n",
    "plt.title('Classifier Log Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad27d0a-ff36-48a3-ac81-f454819a6a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image bar charts for Accuracy and Log Loss\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.style.use('dark_background')\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")\n",
    "plt.xlabel('Accuracy %')\n",
    "plt.title('Classifier Accuracy')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Log Loss', y='Classifier', data=log, color=\"g\")\n",
    "plt.xlabel('Log Loss')\n",
    "plt.title('Classifier Log Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6075ffa-395e-4e76-a121-2c35f23922dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
